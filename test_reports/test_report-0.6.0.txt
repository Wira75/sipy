
Executing script file: C:\Dropbox\MyProjects\sipy\test_scripts\all-test.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\all-test.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\anova.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\data_values.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\compute_effsize.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\data_values.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\correlate.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\data_values.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\describe.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\data_values.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\mean.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\data_values.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\normality.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\data_values.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\regression.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\data_values.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\ttest.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\data_values.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\variance.sipy


Reading script file: C:\Dropbox\MyProjects\sipy\test_scripts\data_values.sipy

Command #1: let yN be clist 1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5
yN = [1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5]
Command #2: let yB be dlist 1, 0, 1, 0, 1, 0, 1, 1, 0, 1
yB = [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]
Command #3: let yC be slist A, B, C, A, B, C, A, B, C, A
yC = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A']
Command #4: let x1 be clist 2, 3, 5, 7, 11, 13, 17, 19, 23, 29
x1 = [2.0, 3.0, 5.0, 7.0, 11.0, 13.0, 17.0, 19.0, 23.0, 29.0]
Command #5: let x2 be clist 1, 4, 9, 16, 25, 36, 49, 64, 81, 100
x2 = [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]
Command #6: let x3 be clist 5, 8, 6, 10, 12, 14, 18, 20, 24, 30
x3 = [5.0, 8.0, 6.0, 10.0, 12.0, 14.0, 18.0, 20.0, 24.0, 30.0]
Command #7: let x4 be clist 3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3
x4 = [3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3]
Command #8: let x5 be clist 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
x5 = [100.0, 90.0, 80.0, 70.0, 60.0, 50.0, 40.0, 30.0, 20.0, 10.0]
Command #9: let df be dataframe yN:yN yB:yB yC:yC x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df = ['yN:yN', 'yB:yB', 'yC:yC', 'x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #10: let df_A be dataframe x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df_A = ['x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #11: let df_B be dataframe x1:x1 x2:x2
df_B = ['x1:x1', 'x2:x2']
Command #12: anova 1way list x1 x2 x3 x4 x5
F = 9.260; p-value = 1.5103829009964831e-05
Command #13: anova 1way list data=x1,x2,x3,x4,x5
F = 9.260; p-value = 1.5103829009964831e-05
Command #14: anova 1way dataframe wide data=df_A
F = 9.260; p-value = 1.5103829009964831e-05
Command #15: anova rm dataframe wide data=df_A
   Source  ddof1  ddof2  ...  sphericity       W-spher       p-spher
0  Within      4     36  ...       False  2.502333e-07  2.248358e-19

[1 rows x 11 columns]
Command #16: let yN be clist 1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5
yN = [1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5]
Command #17: let yB be dlist 1, 0, 1, 0, 1, 0, 1, 1, 0, 1
yB = [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]
Command #18: let yC be slist A, B, C, A, B, C, A, B, C, A
yC = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A']
Command #19: let x1 be clist 2, 3, 5, 7, 11, 13, 17, 19, 23, 29
x1 = [2.0, 3.0, 5.0, 7.0, 11.0, 13.0, 17.0, 19.0, 23.0, 29.0]
Command #20: let x2 be clist 1, 4, 9, 16, 25, 36, 49, 64, 81, 100
x2 = [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]
Command #21: let x3 be clist 5, 8, 6, 10, 12, 14, 18, 20, 24, 30
x3 = [5.0, 8.0, 6.0, 10.0, 12.0, 14.0, 18.0, 20.0, 24.0, 30.0]
Command #22: let x4 be clist 3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3
x4 = [3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3]
Command #23: let x5 be clist 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
x5 = [100.0, 90.0, 80.0, 70.0, 60.0, 50.0, 40.0, 30.0, 20.0, 10.0]
Command #24: let df be dataframe yN:yN yB:yB yC:yC x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df = ['yN:yN', 'yB:yB', 'yC:yC', 'x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #25: let df_A be dataframe x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df_A = ['x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #26: let df_B be dataframe x1:x1 x2:x2
df_B = ['x1:x1', 'x2:x2']
Command #27: compute_effsize auc list x1 x2
0.23444390858839936
Command #28: compute_effsize auc list data=x1,x2
0.23444390858839936
Command #29: compute_effsize auc dataframe wide df_B x1 x2
0.23444390858839936
Command #30: compute_effsize auc dataframe wide data=df_B.x1,df_B.x2
0.23444390858839936
Command #31: compute_effsize cles list x1 x2
0.29
Command #32: compute_effsize cles list data=x1,x2
0.29
Command #33: compute_effsize cles dataframe wide df_B x1 x2
0.29
Command #34: compute_effsize cles dataframe wide data=df_B.x1,df_B.x2
0.29
Command #35: compute_effsize cohen list x1 x2
-1.024300505562247
Command #36: compute_effsize cohen list data=x1,x2
-1.024300505562247
Command #37: compute_effsize cohen dataframe wide df_B x1 x2
-1.024300505562247
Command #38: compute_effsize cohen dataframe wide data=df_B.x1,df_B.x2
-1.024300505562247
Command #39: compute_effsize eta-square list x1 x2
0.20779396470817027
Command #40: compute_effsize eta-square list data=x1,x2
0.20779396470817027
Command #41: compute_effsize eta-square dataframe wide df_B x1 x2
0.20779396470817027
Command #42: compute_effsize eta-square dataframe wide data=df_B.x1,df_B.x2
0.20779396470817027
Command #43: compute_effsize hedges list x1 x2
-0.9810202025103211
Command #44: compute_effsize hedges list data=x1,x2
-0.9810202025103211
Command #45: compute_effsize hedges dataframe wide df_B x1 x2
-0.9810202025103211
Command #46: compute_effsize hedges dataframe wide data=df_B.x1,df_B.x2
-0.9810202025103211
Command #47: compute_effsize none list x1 x2
-1.024300505562247
Command #48: compute_effsize none list data=x1,x2
-1.024300505562247
Command #49: compute_effsize none dataframe wide df_B x1 x2
-1.024300505562247
Command #50: compute_effsize none dataframe wide data=df_B.x1,df_B.x2
-1.024300505562247
Command #51: compute_effsize odds-ratio list x1 x2
0.15600369193329172
Command #52: compute_effsize odds-ratio list data=x1,x2
0.15600369193329172
Command #53: compute_effsize odds-ratio dataframe wide df_B x1 x2
0.15600369193329172
Command #54: compute_effsize odds-ratio dataframe wide data=df_B.x1,df_B.x2
0.15600369193329172
Command #55: compute_effsize r list x1 x2
0.9942517161452283
Command #56: compute_effsize r list data=x1,x2
0.9942517161452283
Command #57: compute_effsize r dataframe wide df_B x1 x2
0.9942517161452283
Command #58: compute_effsize r dataframe wide data=df_B.x1,df_B.x2
0.9942517161452283
Command #59: let yN be clist 1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5
yN = [1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5]
Command #60: let yB be dlist 1, 0, 1, 0, 1, 0, 1, 1, 0, 1
yB = [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]
Command #61: let yC be slist A, B, C, A, B, C, A, B, C, A
yC = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A']
Command #62: let x1 be clist 2, 3, 5, 7, 11, 13, 17, 19, 23, 29
x1 = [2.0, 3.0, 5.0, 7.0, 11.0, 13.0, 17.0, 19.0, 23.0, 29.0]
Command #63: let x2 be clist 1, 4, 9, 16, 25, 36, 49, 64, 81, 100
x2 = [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]
Command #64: let x3 be clist 5, 8, 6, 10, 12, 14, 18, 20, 24, 30
x3 = [5.0, 8.0, 6.0, 10.0, 12.0, 14.0, 18.0, 20.0, 24.0, 30.0]
Command #65: let x4 be clist 3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3
x4 = [3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3]
Command #66: let x5 be clist 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
x5 = [100.0, 90.0, 80.0, 70.0, 60.0, 50.0, 40.0, 30.0, 20.0, 10.0]
Command #67: let df be dataframe yN:yN yB:yB yC:yC x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df = ['yN:yN', 'yB:yB', 'yC:yC', 'x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #68: let df_A be dataframe x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df_A = ['x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #69: let df_B be dataframe x1:x1 x2:x2
df_B = ['x1:x1', 'x2:x2']
Command #70: correlate bicor list x1 x2
        n         r        CI95%         p-val     power
bicor  10  0.987391  [0.95, 1.0]  1.089362e-07  0.999999
Command #71: correlate bicor list data=x1,x2
        n         r        CI95%         p-val     power
bicor  10  0.987391  [0.95, 1.0]  1.089362e-07  0.999999
Command #72: correlate bicor dataframe wide df_B x1 x2
        n         r        CI95%         p-val     power
bicor  10  0.987391  [0.95, 1.0]  1.089362e-07  0.999999
Command #73: correlate bicor dataframe wide data=df_B.x1,df_B.x2
        n         r        CI95%         p-val     power
bicor  10  0.987391  [0.95, 1.0]  1.089362e-07  0.999999
Command #74: correlate distance list x1 x2
          n         r        CI95%         p-val      BF10  power
pearson  10  0.994252  [0.98, 1.0]  2.371931e-09  8.99e+05    1.0
Command #75: correlate distance list data=x1,x2
          n         r        CI95%         p-val      BF10  power
pearson  10  0.994252  [0.98, 1.0]  2.371931e-09  8.99e+05    1.0
Command #76: correlate distance dataframe wide df_B x1 x2
          n         r        CI95%         p-val      BF10  power
pearson  10  0.994252  [0.98, 1.0]  2.371931e-09  8.99e+05    1.0
Command #77: correlate distance dataframe wide data=df_B.x1,df_B.x2
          n         r        CI95%         p-val      BF10  power
pearson  10  0.994252  [0.98, 1.0]  2.371931e-09  8.99e+05    1.0
Command #78: correlate kendall list x1 x2
          n    r       CI95%         p-val  power
kendall  10  1.0  [1.0, 1.0]  5.511464e-07    1.0
Command #79: correlate kendall list data=x1,x2
          n    r       CI95%         p-val  power
kendall  10  1.0  [1.0, 1.0]  5.511464e-07    1.0
Command #80: correlate kendall dataframe wide df_B x1 x2
          n    r       CI95%         p-val  power
kendall  10  1.0  [1.0, 1.0]  5.511464e-07    1.0
Command #81: correlate kendall dataframe wide data=df_B.x1,df_B.x2
          n    r       CI95%         p-val  power
kendall  10  1.0  [1.0, 1.0]  5.511464e-07    1.0
Command #82: correlate pearson list x1 x2
          n         r        CI95%         p-val       BF10  power
pearson  10  0.994252  [0.97, 1.0]  4.743861e-09  4.495e+05    1.0
Command #83: correlate pearson list data=x1,x2
          n         r        CI95%         p-val       BF10  power
pearson  10  0.994252  [0.97, 1.0]  4.743861e-09  4.495e+05    1.0
Command #84: correlate pearson dataframe wide df_B x1 x2
          n         r        CI95%         p-val       BF10  power
pearson  10  0.994252  [0.97, 1.0]  4.743861e-09  4.495e+05    1.0
Command #85: correlate pearson dataframe wide data=df_B.x1,df_B.x2
          n         r        CI95%         p-val       BF10  power
pearson  10  0.994252  [0.97, 1.0]  4.743861e-09  4.495e+05    1.0
Command #86: correlate percbend list x1 x2
           n         r        CI95%         p-val  power
percbend  10  0.993952  [0.97, 1.0]  5.813106e-09    1.0
Command #87: correlate percbend list data=x1,x2
           n         r        CI95%         p-val  power
percbend  10  0.993952  [0.97, 1.0]  5.813106e-09    1.0
Command #88: correlate percbend dataframe wide df_B x1 x2
           n         r        CI95%         p-val  power
percbend  10  0.993952  [0.97, 1.0]  5.813106e-09    1.0
Command #89: correlate percbend dataframe wide data=df_B.x1,df_B.x2
           n         r        CI95%         p-val  power
percbend  10  0.993952  [0.97, 1.0]  5.813106e-09    1.0
Command #90: correlate skipped list x1 x2
          n  outliers    r       CI95%         p-val  power
skipped  10         0  1.0  [1.0, 1.0]  6.646897e-64    1.0
Command #91: correlate skipped list data=x1,x2
          n  outliers    r       CI95%         p-val  power
skipped  10         0  1.0  [1.0, 1.0]  6.646897e-64    1.0
Command #92: correlate skipped dataframe wide df_B x1 x2
          n  outliers    r       CI95%         p-val  power
skipped  10         0  1.0  [1.0, 1.0]  6.646897e-64    1.0
Command #93: correlate skipped dataframe wide data=df_B.x1,df_B.x2
          n  outliers    r       CI95%         p-val  power
skipped  10         0  1.0  [1.0, 1.0]  6.646897e-64    1.0
Command #94: correlate spearman list x1 x2
           n    r       CI95%         p-val  power
spearman  10  1.0  [1.0, 1.0]  6.646897e-64    1.0
Command #95: correlate spearman list data=x1,x2
           n    r       CI95%         p-val  power
spearman  10  1.0  [1.0, 1.0]  6.646897e-64    1.0
Command #96: correlate spearman dataframe wide df_B x1 x2
           n    r       CI95%         p-val  power
spearman  10  1.0  [1.0, 1.0]  6.646897e-64    1.0
Command #97: correlate spearman dataframe wide data=df_B.x1,df_B.x2
           n    r       CI95%         p-val  power
spearman  10  1.0  [1.0, 1.0]  6.646897e-64    1.0
Command #98: let yN be clist 1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5
yN = [1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5]
Command #99: let yB be dlist 1, 0, 1, 0, 1, 0, 1, 1, 0, 1
yB = [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]
Command #100: let yC be slist A, B, C, A, B, C, A, B, C, A
yC = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A']
Command #101: let x1 be clist 2, 3, 5, 7, 11, 13, 17, 19, 23, 29
x1 = [2.0, 3.0, 5.0, 7.0, 11.0, 13.0, 17.0, 19.0, 23.0, 29.0]
Command #102: let x2 be clist 1, 4, 9, 16, 25, 36, 49, 64, 81, 100
x2 = [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]
Command #103: let x3 be clist 5, 8, 6, 10, 12, 14, 18, 20, 24, 30
x3 = [5.0, 8.0, 6.0, 10.0, 12.0, 14.0, 18.0, 20.0, 24.0, 30.0]
Command #104: let x4 be clist 3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3
x4 = [3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3]
Command #105: let x5 be clist 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
x5 = [100.0, 90.0, 80.0, 70.0, 60.0, 50.0, 40.0, 30.0, 20.0, 10.0]
Command #106: let df be dataframe yN:yN yB:yB yC:yC x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df = ['yN:yN', 'yB:yB', 'yC:yC', 'x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #107: let df_A be dataframe x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df_A = ['x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #108: let df_B be dataframe x1:x1 x2:x2
df_B = ['x1:x1', 'x2:x2']
Command #109: describe kurtosis x1
Kurtosis = -0.9874131145220866
Command #110: describe kurtosis data=x1
Kurtosis = -0.9874131145220866
Command #111: describe se x1
Standard error = 2.853652630109932
Command #112: describe se data=x1
Standard error = 2.853652630109932
Command #113: describe skew x1
Skew = 0.4030520544765256
Command #114: describe skew data=x1
Skew = 0.4030520544765256
Command #115: describe stdev x1
Standard deviation = 9.024041962077378
Command #116: describe stdev data=x1
Standard deviation = 9.024041962077378
Command #117: describe var x1
Variance = 81.43333333333332
Command #118: describe var data=x1
Variance = 81.43333333333332
Command #119: let yN be clist 1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5
yN = [1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5]
Command #120: let yB be dlist 1, 0, 1, 0, 1, 0, 1, 1, 0, 1
yB = [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]
Command #121: let yC be slist A, B, C, A, B, C, A, B, C, A
yC = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A']
Command #122: let x1 be clist 2, 3, 5, 7, 11, 13, 17, 19, 23, 29
x1 = [2.0, 3.0, 5.0, 7.0, 11.0, 13.0, 17.0, 19.0, 23.0, 29.0]
Command #123: let x2 be clist 1, 4, 9, 16, 25, 36, 49, 64, 81, 100
x2 = [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]
Command #124: let x3 be clist 5, 8, 6, 10, 12, 14, 18, 20, 24, 30
x3 = [5.0, 8.0, 6.0, 10.0, 12.0, 14.0, 18.0, 20.0, 24.0, 30.0]
Command #125: let x4 be clist 3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3
x4 = [3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3]
Command #126: let x5 be clist 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
x5 = [100.0, 90.0, 80.0, 70.0, 60.0, 50.0, 40.0, 30.0, 20.0, 10.0]
Command #127: let df be dataframe yN:yN yB:yB yC:yC x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df = ['yN:yN', 'yB:yB', 'yC:yC', 'x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #128: let df_A be dataframe x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df_A = ['x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #129: let df_B be dataframe x1:x1 x2:x2
df_B = ['x1:x1', 'x2:x2']
Command #130: mean arithmetic x1
Arimethic mean = 12.9
Command #131: mean arithmetic data=x1
Arimethic mean = 12.9
Command #132: mean geometric x1
Geometric mean = 9.573888578831271
Command #133: mean geometric data=x1
Geometric mean = 9.573888578831271
Command #134: mean harmonic x1
Harmonic mean = 6.521290698677154
Command #135: mean harmonic data=x1
Harmonic mean = 6.521290698677154
Command #136: let yN be clist 1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5
yN = [1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5]
Command #137: let yB be dlist 1, 0, 1, 0, 1, 0, 1, 1, 0, 1
yB = [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]
Command #138: let yC be slist A, B, C, A, B, C, A, B, C, A
yC = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A']
Command #139: let x1 be clist 2, 3, 5, 7, 11, 13, 17, 19, 23, 29
x1 = [2.0, 3.0, 5.0, 7.0, 11.0, 13.0, 17.0, 19.0, 23.0, 29.0]
Command #140: let x2 be clist 1, 4, 9, 16, 25, 36, 49, 64, 81, 100
x2 = [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]
Command #141: let x3 be clist 5, 8, 6, 10, 12, 14, 18, 20, 24, 30
x3 = [5.0, 8.0, 6.0, 10.0, 12.0, 14.0, 18.0, 20.0, 24.0, 30.0]
Command #142: let x4 be clist 3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3
x4 = [3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3]
Command #143: let x5 be clist 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
x5 = [100.0, 90.0, 80.0, 70.0, 60.0, 50.0, 40.0, 30.0, 20.0, 10.0]
Command #144: let df be dataframe yN:yN yB:yB yC:yC x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df = ['yN:yN', 'yB:yB', 'yC:yC', 'x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #145: let df_A be dataframe x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df_A = ['x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #146: let df_B be dataframe x1:x1 x2:x2
df_B = ['x1:x1', 'x2:x2']
Command #147: normality kurtosis x1
Z-score = -0.5122942701282507; p-value = 0.608445074800392
Command #148: normality kurtosis data=x1
Z-score = -0.5122942701282507; p-value = 0.608445074800392
Command #149: normality kurtosis df_A
Z-score = [-0.51229427 -0.47723698 -0.18472672 -1.16071509 -0.98906474]; p-value = [0.60844507 0.63319338 0.85344339 0.24575778 0.32263147]
Command #150: normality kurtosis data=df_A
Z-score = [-0.51229427 -0.47723698 -0.18472672 -1.16071509 -0.98906474]; p-value = [0.60844507 0.63319338 0.85344339 0.24575778 0.32263147]
Command #151: normality jarquebera x1
Statistic = 0.6769952055005001; p-value = 0.7128404882435215
Command #152: normality jarquebera data=x1
Statistic = 0.6769952055005001; p-value = 0.7128404882435215
Command #153: normality jarquebera df_A
Statistic = 19.904084848945228; p-value = 4.763025332934925e-05
Command #154: normality jarquebera data=df_A
Statistic = 19.904084848945228; p-value = 4.763025332934925e-05
Command #155: normality shapirowilk x1
Statistic = 0.949425736005048; p-value = 0.6617106334001631
Command #156: normality shapirowilk data=x1
Statistic = 0.949425736005048; p-value = 0.6617106334001631
Command #157: normality shapirowilk df_A
Statistic = 0.7771954045321356; p-value = 2.7184402555824915e-07
Command #158: normality shapirowilk data=df_A
Statistic = 0.7771954045321356; p-value = 2.7184402555824915e-07
Command #159: normality skewtest x1
Z-score = 0.716977; p-value = 0.473388
Command #160: normality skewtest data=x1
Z-score = 0.716977; p-value = 0.473388
Command #161: normality skewtest df_A
Z-score, p-value 
0.7169774652205027, 0.47338800143151827 
1.0056270445744642, 0.3145950093890053 
0.9705928134120045, 0.33175108609595383 
-0.07684093862381985, 0.9387500826844087 
1.024073604571085, 0.3058005106397915
Command #162: normality skewtest data=df_A
Z-score, p-value 
0.7169774652205027, 0.47338800143151827 
1.0056270445744642, 0.3145950093890053 
0.9705928134120045, 0.33175108609595383 
-0.07684093862381985, 0.9387500826844087 
1.024073604571085, 0.3058005106397915
Command #163: let yN be clist 1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5
yN = [1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5]
Command #164: let yB be dlist 1, 0, 1, 0, 1, 0, 1, 1, 0, 1
yB = [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]
Command #165: let yC be slist A, B, C, A, B, C, A, B, C, A
yC = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A']
Command #166: let x1 be clist 2, 3, 5, 7, 11, 13, 17, 19, 23, 29
x1 = [2.0, 3.0, 5.0, 7.0, 11.0, 13.0, 17.0, 19.0, 23.0, 29.0]
Command #167: let x2 be clist 1, 4, 9, 16, 25, 36, 49, 64, 81, 100
x2 = [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]
Command #168: let x3 be clist 5, 8, 6, 10, 12, 14, 18, 20, 24, 30
x3 = [5.0, 8.0, 6.0, 10.0, 12.0, 14.0, 18.0, 20.0, 24.0, 30.0]
Command #169: let x4 be clist 3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3
x4 = [3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3]
Command #170: let x5 be clist 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
x5 = [100.0, 90.0, 80.0, 70.0, 60.0, 50.0, 40.0, 30.0, 20.0, 10.0]
Command #171: let df be dataframe yN:yN yB:yB yC:yC x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df = ['yN:yN', 'yB:yB', 'yC:yC', 'x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #172: let df_A be dataframe x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df_A = ['x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #173: let df_B be dataframe x1:x1 x2:x2
df_B = ['x1:x1', 'x2:x2']
Command #174: rregress cloglog data=df y=yB x=x1,x2,x3,x4,x5
Call:
glm(formula = yB ~ x1 + x2 + x3 + x4 + x5, family = binomial(link = "cloglog"), 
    data = data)

Coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept) -37.42995   36.59558  -1.023    0.306
x1            3.59106    3.23870   1.109    0.268
x2           -0.09291    0.21418  -0.434    0.664
x3           -1.90586    2.19276  -0.869    0.385
x4           -0.08588    0.45361  -0.189    0.850
x5            0.41811    0.38778   1.078    0.281

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 13.4602  on 9  degrees of freedom
Residual deviance:  5.8472  on 4  degrees of freedom
AIC: 17.847

Number of Fisher Scoring iterations: 11
Command #175: rregress cloglog data=df y=yB x=all
Call:
glm(formula = yB ~ yN + yC + x1 + x2 + x3 + x4 + x5, family = binomial(link = "cloglog"), 
    data = data)

Coefficients:
              Estimate Std. Error z value Pr(>|z|)
(Intercept) -3.997e+02  5.220e+06       0        1
yN           2.075e+01  2.948e+05       0        1
yCB          5.456e+00  1.816e+05       0        1
yCC         -2.224e+01  1.788e+05       0        1
x1           1.843e+01  6.196e+04       0        1
x2           1.538e+00  1.037e+04       0        1
x3          -1.708e+01  1.010e+05       0        1
x4          -3.225e+00  4.000e+04       0        1
x5           4.576e+00  4.735e+04       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.3460e+01  on 9  degrees of freedom
Residual deviance: 4.6972e-10  on 1  degrees of freedom
AIC: 18

Number of Fisher Scoring iterations: 23
Command #176: rregress decision_tree data=df y=yN x=x1,x2,x3,x4,x5
Call:
rpart::rpart(formula = yN ~ x1 + x2 + x3 + x4 + x5, data = data)
  n= 10 

    CP nsplit rel error xerror xstd
1 0.01      0         1      0    0

Node number 1: 10 observations
  mean=5.97, MSE=9.0081
Command #177: rregress decision_tree data=df y=yN x=all
Call:
rpart::rpart(formula = yN ~ yB + yC + x1 + x2 + x3 + x4 + x5, 
    data = data)
  n= 10 

    CP nsplit rel error xerror xstd
1 0.01      0         1      0    0

Node number 1: 10 observations
  mean=5.97, MSE=9.0081
Command #178: rregress elasticnet data=df y=yN x=x1,x2,x3,x4,x5
Length Class  Mode     
lambda     62     -none- numeric  
cvm        62     -none- numeric  
cvsd       62     -none- numeric  
cvup       62     -none- numeric  
cvlo       62     -none- numeric  
nzero      62     -none- numeric  
call        4     -none- call     
name        1     -none- character
glmnet.fit 12     elnet  list     
lambda.min  1     -none- numeric  
lambda.1se  1     -none- numeric  
index       2     -none- numeric
Command #179: rregress elasticnet data=df y=yN x=all
Length Class  Mode     
lambda     95     -none- numeric  
cvm        95     -none- numeric  
cvsd       95     -none- numeric  
cvup       95     -none- numeric  
cvlo       95     -none- numeric  
nzero      95     -none- numeric  
call        4     -none- call     
name        1     -none- character
glmnet.fit 12     elnet  list     
lambda.min  1     -none- numeric  
lambda.1se  1     -none- numeric  
index       2     -none- numeric
Command #180: rregress gamma data=df y=yN x=x1,x2,x3,x4,x5
Call:
glm(formula = yN ~ x1 + x2 + x3 + x4 + x5, family = Gamma(link = "log"), 
    data = data)

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  5.442789   0.624957   8.709 0.000957 ***
x1          -0.035289   0.046439  -0.760 0.489643    
x2          -0.025587   0.010460  -2.446 0.070739 .  
x3           0.022213   0.038904   0.571 0.598569    
x4           0.025924   0.018197   1.425 0.227379    
x5          -0.052692   0.006253  -8.427 0.001086 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for Gamma family taken to be 0.007316107)

    Null deviance: 3.572618  on 9  degrees of freedom
Residual deviance: 0.028706  on 4  degrees of freedom
AIC: 16.014

Number of Fisher Scoring iterations: 4
Command #181: rregress gamma data=df y=yN x=all
Call:
glm(formula = yN ~ yB + yC + x1 + x2 + x3 + x4 + x5, family = Gamma(link = "log"), 
    data = data)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  5.89138    1.11911   5.264    0.120
yB           0.28165    0.24312   1.158    0.453
yCB         -0.05730    0.09869  -0.581    0.665
yCC          0.25788    0.21492   1.200    0.442
x1          -0.18414    0.14366  -1.282    0.422
x2          -0.03478    0.01499  -2.321    0.259
x3           0.17116    0.12858   1.331    0.410
x4           0.04152    0.02568   1.617    0.353
x5          -0.06545    0.01484  -4.411    0.142

(Dispersion parameter for Gamma family taken to be 0.01095669)

    Null deviance: 3.572618  on 9  degrees of freedom
Residual deviance: 0.011043  on 1  degrees of freedom
AIC: 12.457

Number of Fisher Scoring iterations: 4
Command #182: rregress gradient_boosting data=df y=yN x=x1,x2,x3,x4,x5
Distribution not specified, assuming gaussian ...
   var    rel.inf
x1  x1 91.4941431
x5  x5  7.6557147
x4  x4  0.8501423
x2  x2  0.0000000
x3  x3  0.0000000
Command #183: rregress hurdle data=df y=yB x=x1,x2,x3,x4,x5
Call:
pscl::hurdle(formula = yB ~ x1 + x2 + x3 + x4 + x5, data = data)

Pearson residuals:
     Min       1Q   Median       3Q      Max 
-1.26969 -0.09179  0.11949  0.32417  1.56110 

Count model coefficients (truncated poisson with log link):
              Estimate Std. Error z value Pr(>|z|)
(Intercept) -1.305e+01  3.380e+06       0        1
x1           1.426e+00  5.401e+05       0        1
x2          -2.349e-01  3.567e+04       0        1
x3          -9.312e-01  4.655e+05       0        1
x4          -9.479e-02  5.754e+04       0        1
x5          -1.098e-01  3.782e+04       0        1
Zero hurdle model coefficients (binomial with logit link):
             Estimate Std. Error z value Pr(>|z|)
(Intercept) -47.49114   48.74507  -0.974    0.330
x1            4.46216    3.85487   1.158    0.247
x2           -0.07283    0.38649  -0.188    0.851
x3           -2.45155    3.26683  -0.750    0.453
x4           -0.16344    0.73701  -0.222    0.825
x5            0.55247    0.51536   1.072    0.284

Number of iterations in BFGS optimization: 4 
Log-likelihood: -3.198 on 12 Df
Command #184: rregress hurdle data=df y=yB x=all
Call:
pscl::hurdle(formula = yB ~ yN + yC + x1 + x2 + x3 + x4 + x5, data = data)

Pearson residuals:
       Min         1Q     Median         3Q        Max 
-4.860e-06 -2.369e-06  1.691e-07  2.904e-06  5.332e-06 

Count model coefficients (truncated poisson with log link):
              Estimate Std. Error z value Pr(>|z|)
(Intercept)  8.709e+01        NaN     NaN      NaN
yN          -5.598e+00  1.125e+06       0        1
yCB         -1.107e+01        NaN     NaN      NaN
yCC         -3.446e+01        NaN     NaN      NaN
x1           1.912e+01        NaN     NaN      NaN
x2           1.793e+00        NaN     NaN      NaN
x3          -2.663e+01        NaN     NaN      NaN
x4           6.113e-01        NaN     NaN      NaN
x5          -1.994e-01  3.756e+04       0        1
Zero hurdle model coefficients (binomial with logit link):
              Estimate Std. Error z value Pr(>|z|)
(Intercept) -7.467e+02  1.475e+07       0        1
yN           3.993e+01  9.789e+05       0        1
yCB          1.167e+01  4.674e+05       0        1
yCC         -4.108e+01  4.943e+05       0        1
x1           3.369e+01  2.557e+05       0        1
x2           2.848e+00  4.536e+04       0        1
x3          -3.096e+01  1.961e+05       0        1
x4          -5.995e+00  1.276e+05       0        1
x5           8.700e+00  1.358e+05       0        1

Number of iterations in BFGS optimization: 3 
Log-likelihood: 0.003484 on 18 Df
Command #185: rregress inversegaussian data=df y=yN x=x1,x2,x3,x4,x5
Call:
glm(formula = yN ~ x1 + x2 + x3 + x4 + x5, family = inverse.gaussian(link = "log"), 
    data = data)

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  6.240421   0.760719   8.203 0.001203 ** 
x1          -0.068109   0.059039  -1.154 0.312894    
x2          -0.024969   0.014570  -1.714 0.161727    
x3           0.027999   0.037078   0.755 0.492189    
x4           0.020467   0.023227   0.881 0.427993    
x5          -0.060689   0.006775  -8.958 0.000859 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for inverse.gaussian family taken to be 0.001987347)

    Null deviance: 0.9078106  on 9  degrees of freedom
Residual deviance: 0.0076893  on 4  degrees of freedom
AIC: 18.917

Number of Fisher Scoring iterations: 5
Command #186: rregress inversegaussian data=df y=yN x=all
Call:
glm(formula = yN ~ yB + yC + x1 + x2 + x3 + x4 + x5, family = inverse.gaussian(link = "log"), 
    data = data)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  6.14609    0.96856   6.346   0.0995 .
yB           0.32049    0.18333   1.748   0.3308  
yCB         -0.06826    0.08700  -0.785   0.5765  
yCC          0.29859    0.14612   2.044   0.2897  
x1          -0.21245    0.10710  -1.984   0.2973  
x2          -0.03513    0.01367  -2.570   0.2362  
x3           0.18903    0.09399   2.011   0.2938  
x4           0.04592    0.02289   2.006   0.2944  
x5          -0.06908    0.01129  -6.116   0.1032  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for inverse.gaussian family taken to be 0.001421984)

    Null deviance: 0.9078106  on 9  degrees of freedom
Residual deviance: 0.0014522  on 1  degrees of freedom
AIC: 8.2493

Number of Fisher Scoring iterations: 4
Command #187: rregress lasso data=df y=yN x=x1,x2,x3,x4,x5
Length Class  Mode     
lambda     56     -none- numeric  
cvm        56     -none- numeric  
cvsd       56     -none- numeric  
cvup       56     -none- numeric  
cvlo       56     -none- numeric  
nzero      56     -none- numeric  
call        4     -none- call     
name        1     -none- character
glmnet.fit 12     elnet  list     
lambda.min  1     -none- numeric  
lambda.1se  1     -none- numeric  
index       2     -none- numeric
Command #188: rregress lasso data=df y=yN x=all
Length Class  Mode     
lambda     75     -none- numeric  
cvm        75     -none- numeric  
cvsd       75     -none- numeric  
cvup       75     -none- numeric  
cvlo       75     -none- numeric  
nzero      75     -none- numeric  
call        4     -none- call     
name        1     -none- character
glmnet.fit 12     elnet  list     
lambda.min  1     -none- numeric  
lambda.1se  1     -none- numeric  
index       2     -none- numeric
Command #189: rregress lm data=df y=yN x=x1,x2,x3,x4,x5
Call:
lm(formula = yN ~ x1 + x2 + x3 + x4 + x5, data = data)

Residuals:
       1        2        3        4        5        6        7        8 
 0.11440 -0.08477 -0.05909  0.05514 -0.15369 -0.12513  0.32204 -0.00958 
       9       10 
 0.05335 -0.11267 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept) 12.04579    1.57314   7.657  0.00156 **
x1          -0.02335    0.11690  -0.200  0.85141   
x2          -0.01062    0.02633  -0.403  0.70740   
x3           0.01416    0.09793   0.145  0.89199   
x4           0.08901    0.04580   1.943  0.12391   
x5          -0.11250    0.01574  -7.147  0.00203 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.2153 on 4 degrees of freedom
Multiple R-squared:  0.9979,	Adjusted R-squared:  0.9954 
F-statistic: 387.8 on 5 and 4 DF,  p-value: 1.85e-05
Command #190: rregress lm data=df y=yN x=all
Call:
lm(formula = yN ~ yB + yC + x1 + x2 + x3 + x4 + x5, data = data)

Residuals:
        1         2         3         4         5         6         7         8 
 0.074350  0.012392 -0.086742 -0.099133  0.007875 -0.040534  0.156575 -0.020267 
        9        10 
 0.127275 -0.131792 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) 14.22103    3.08447   4.611    0.136
yB           0.24707    0.67008   0.369    0.775
yCB         -0.27691    0.27200  -1.018    0.494
yCC         -0.06459    0.59235  -0.109    0.931
x1          -0.21814    0.39594  -0.551    0.679
x2          -0.00765    0.04131  -0.185    0.883
x3           0.12105    0.35438   0.342    0.790
x4           0.08859    0.07078   1.252    0.429
x5          -0.13778    0.04090  -3.369    0.184

Residual standard error: 0.2885 on 1 degrees of freedom
Multiple R-squared:  0.9991,	Adjusted R-squared:  0.9917 
F-statistic: 135.2 on 8 and 1 DF,  p-value: 0.06643
Command #191: rregress multinom data=df y=yC x=x1,x2,x3,x4,x5
# weights:  21 (12 variable)
initial  value 10.986123 
iter  10 value 7.795260
iter  20 value 6.740264
iter  30 value 6.623106
iter  40 value 6.316542
iter  50 value 6.215245
iter  60 value 6.204664
iter  70 value 6.120127
iter  80 value 5.967442
iter  90 value 5.877778
iter 100 value 5.839955
final  value 5.839955 
stopped after 100 iterations
Call:
nnet::multinom(formula = yC ~ x1 + x2 + x3 + x4 + x5, data = data)

Coefficients:
  (Intercept)        x1         x2          x3         x4         x5
B    31.14076 -1.349844 0.04034069   0.2521877 -0.3572707 -0.3012964
C   -51.08582 68.881412 3.09595680 -75.6129292 -1.2046572  2.2135126

Std. Errors:
  (Intercept)       x1        x2       x3        x4        x5
B  30.5607166 1.467528 0.3508552 1.329338 0.6710372 0.2714155
C   0.3259539 2.341665 1.2324099 2.265220 0.8871705 0.3343897

Residual Deviance: 11.67991 
AIC: 35.67991
Command #192: rregress multinom data=df y=yC x=all
# weights:  27 (16 variable)
initial  value 10.986123 
iter  10 value 4.987441
iter  20 value 0.128868
iter  30 value 0.000126
iter  30 value 0.000063
iter  30 value 0.000062
final  value 0.000062 
converged
Call:
nnet::multinom(formula = yC ~ yN + yB + x1 + x2 + x3 + x4 + x5, 
    data = data)

Coefficients:
  (Intercept)       yN        yB        x1        x2         x3        x4
B    60.24454 36.15776  291.9850 -153.5995  5.865259   95.96562 27.680136
C    11.68283 74.27455 -445.9784  166.4057 40.496512 -331.64982  1.921081
         x5
B -6.716137
C 14.284189

Std. Errors:
   (Intercept)           yN           yB           x1           x2           x3
B 4.464659e+01 3.040910e+02 2.102667e+02 3.991768e+02 9.966404e+01 3.009113e+02
C 1.204282e-52 5.780552e-52 2.786879e-83 8.429972e-52 1.926851e-51 1.204282e-51
            x4           x5
B 2.180891e+02 1.015285e+01
C 1.035682e-51 8.429972e-51

Residual Deviance: 0.0001233243 
AIC: 32.00012
Command #193: rregress negbinom data=df y=yB x=x1,x2,x3,x4,x5
Call:
MASS::glm.nb(formula = yB ~ x1 + x2 + x3 + x4 + x5, data = data, 
    init.theta = 21620.76942, link = log)

Coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept) -13.04331   11.28134  -1.156    0.248
x1            1.48864    1.16366   1.279    0.201
x2           -0.04845    0.18632  -0.260    0.795
x3           -0.86228    0.95054  -0.907    0.364
x4           -0.06348    0.33029  -0.192    0.848
x5            0.14422    0.11636   1.239    0.215

(Dispersion parameter for Negative Binomial(21620.77) family taken to be 1)

    Null deviance: 6.1298  on 9  degrees of freedom
Residual deviance: 3.2119  on 4  degrees of freedom
AIC: 29.212

Number of Fisher Scoring iterations: 1


              Theta:  21621 
          Std. Err.:  781612 
Warning while fitting theta: iteration limit reached 

 2 x log-likelihood:  -15.212
Command #194: rregress negbinom data=df y=yB x=all
Call:
MASS::glm.nb(formula = yB ~ yN + yC + x1 + x2 + x3 + x4 + x5, 
    data = data, init.theta = 47139.34767, link = log)

Coefficients:
              Estimate Std. Error z value Pr(>|z|)
(Intercept)  9.228e+01  9.762e+06       0        1
yN          -5.869e+00  5.624e+05       0        1
yCB         -1.161e+01  3.521e+05       0        1
yCC         -3.598e+01  3.783e+05       0        1
x1           1.998e+01  1.520e+05       0        1
x2           2.116e+00  2.114e+04       0        1
x3          -2.768e+01  1.153e+05       0        1
x4           6.946e-01  7.473e+04       0        1
x5           8.947e-02  8.847e+04       0        1

(Dispersion parameter for Negative Binomial(47139.35) family taken to be 1)

    Null deviance: 6.1299e+00  on 9  degrees of freedom
Residual deviance: 9.4674e-11  on 1  degrees of freedom
AIC: 32

Number of Fisher Scoring iterations: 1


              Theta:  47139 
          Std. Err.:  2274405 
Warning while fitting theta: iteration limit reached 

 2 x log-likelihood:  -12
Command #195: rregress polr data=df y=yC x=x1,x2,x3,x4,x5
Call:
MASS::polr(formula = yC ~ x1 + x2 + x3 + x4 + x5, data = data)

Coefficients:
     Value Std. Error t value
x1 -1.0689     1.1918 -0.8969
x2  0.2302     0.2633  0.8741
x3 -0.7294     0.9270 -0.7869
x4 -0.2134     0.4313 -0.4948
x5 -0.2624     0.1899 -1.3813

Intercepts:
    Value    Std. Error t value 
A|B -32.2249  20.3046    -1.5871
B|C -30.2339  19.9199    -1.5178

Residual Deviance: 16.48028 
AIC: 30.48028
Command #196: rregress polr data=df y=yC x=all
Call:
MASS::polr(formula = yC ~ yN + yB + x1 + x2 + x3 + x4 + x5, data = data)

Coefficients:
     Value Std. Error  t value
yN -605.14    1098.27 -0.55100
yB  -25.46     260.90 -0.09758
x1  -57.20     349.67 -0.16358
x2   12.78     257.39  0.04967
x3  -36.08     992.76 -0.03634
x4   62.74      79.13  0.79291
x5  -70.35      78.84 -0.89231

Intercepts:
    Value      Std. Error t value   
A|B -7816.9967    82.1112   -95.2001
B|C -7792.5689  2324.3775    -3.3525

Residual Deviance: 0.0005399265 
AIC: 18.00054
Command #197: rregress quasibinom data=df y=yB x=x1,x2,x3,x4,x5
Call:
glm(formula = yB ~ x1 + x2 + x3 + x4 + x5, family = quasibinomial(), 
    data = data)

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) -47.49114   56.19609  -0.845    0.446
x1            4.46216    4.43494   1.006    0.371
x2           -0.07283    0.44381  -0.164    0.878
x3           -2.45155    3.75836  -0.652    0.550
x4           -0.16344    0.84769  -0.193    0.857
x5            0.55247    0.59463   0.929    0.405

(Dispersion parameter for quasibinomial family taken to be 1.324672)

    Null deviance: 13.4602  on 9  degrees of freedom
Residual deviance:  6.3964  on 4  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 7
Command #198: rregress quasibinom data=df y=yB x=all
Call:
glm(formula = yB ~ yN + yC + x1 + x2 + x3 + x4 + x5, family = quasibinomial(), 
    data = data)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept) -746.693    159.680  -4.676   0.1341  
yN            39.931     10.586   3.772   0.1650  
yCB           11.667      5.056   2.307   0.2604  
yCC          -41.081      5.346  -7.684   0.0824 .
x1            33.691      2.763  12.192   0.0521 .
x2             2.848      0.491   5.800   0.1087  
x3           -30.959      2.121 -14.599   0.0435 *
x4            -5.995      1.380  -4.345   0.1440  
x5             8.700      1.471   5.914   0.1066  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for quasibinomial family taken to be 3.178174e-10)

    Null deviance: 1.3460e+01  on 9  degrees of freedom
Residual deviance: 2.3384e-10  on 1  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 24
Command #199: rregress quasipoisson data=df y=yN x=x1,x2,x3,x4,x5
Call:
glm(formula = yN ~ x1 + x2 + x3 + x4 + x5, family = quasipoisson(), 
    data = data)

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  4.904432   0.438557  11.183 0.000364 ***
x1          -0.018707   0.035268  -0.530 0.623906    
x2          -0.025424   0.006992  -3.636 0.022038 *  
x3           0.023143   0.036044   0.642 0.555774    
x4           0.025344   0.012478   2.031 0.112071    
x5          -0.047003   0.005150  -9.127 0.000800 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for quasipoisson family taken to be 0.02080112)

    Null deviance: 16.793020  on 9  degrees of freedom
Residual deviance:  0.083447  on 4  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 3
Command #200: rregress quasipoisson data=df y=yN x=all
Call:
glm(formula = yN ~ yB + yC + x1 + x2 + x3 + x4 + x5, family = quasipoisson(), 
    data = data)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  5.36002    1.11545   4.805    0.131
yB           0.16330    0.29020   0.563    0.674
yCB         -0.04186    0.09745  -0.430    0.742
yCC          0.13802    0.28300   0.488    0.711
x1          -0.11195    0.16980  -0.659    0.629
x2          -0.03004    0.01712  -1.754    0.330
x3           0.10819    0.16775   0.645    0.635
x4           0.03275    0.02482   1.319    0.413
x5          -0.05616    0.01747  -3.214    0.192

(Dispersion parameter for quasipoisson family taken to be 0.05810227)

    Null deviance: 16.793020  on 9  degrees of freedom
Residual deviance:  0.058532  on 1  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 3
Command #201: rregress randomforest data=df y=yN x=x1,x2,x3,x4,x5
Length Class  Mode     
call              3    -none- call     
type              1    -none- character
predicted        10    -none- numeric  
mse             500    -none- numeric  
rsq             500    -none- numeric  
oob.times        10    -none- numeric  
importance        5    -none- numeric  
importanceSD      0    -none- NULL     
localImportance   0    -none- NULL     
proximity         0    -none- NULL     
ntree             1    -none- numeric  
mtry              1    -none- numeric  
forest           11    -none- list     
coefs             0    -none- NULL     
y                10    -none- numeric  
test              0    -none- NULL     
inbag             0    -none- NULL     
terms             3    terms  call
Command #202: rregress randomforest data=df y=yN x=all
Length Class  Mode     
call              3    -none- call     
type              1    -none- character
predicted        10    -none- numeric  
mse             500    -none- numeric  
rsq             500    -none- numeric  
oob.times        10    -none- numeric  
importance        7    -none- numeric  
importanceSD      0    -none- NULL     
localImportance   0    -none- NULL     
proximity         0    -none- NULL     
ntree             1    -none- numeric  
mtry              1    -none- numeric  
forest           11    -none- list     
coefs             0    -none- NULL     
y                10    -none- numeric  
test              0    -none- NULL     
inbag             0    -none- NULL     
terms             3    terms  call
Command #203: rregress svm data=df y=yN x=x1,x2,x3,x4,x5
Call:
svm(formula = yN ~ x1 + x2 + x3 + x4 + x5, data = data)


Parameters:
   SVM-Type:  eps-regression 
 SVM-Kernel:  radial 
       cost:  1 
      gamma:  0.2 
    epsilon:  0.1 


Number of Support Vectors:  8
Command #204: rregress svm data=df y=yN x=all
Call:
svm(formula = yN ~ yB + yC + x1 + x2 + x3 + x4 + x5, data = data)


Parameters:
   SVM-Type:  eps-regression 
 SVM-Kernel:  radial 
       cost:  1 
      gamma:  0.1111111 
    epsilon:  0.1 


Number of Support Vectors:  7
Command #205: rregress svr data=df y=yN x=x1,x2,x3,x4,x5
Call:
svm(formula = yN ~ x1 + x2 + x3 + x4 + x5, data = data)


Parameters:
   SVM-Type:  eps-regression 
 SVM-Kernel:  radial 
       cost:  1 
      gamma:  0.2 
    epsilon:  0.1 


Number of Support Vectors:  8
Command #206: rregress svr data=df y=yN x=all
Call:
svm(formula = yN ~ yB + yC + x1 + x2 + x3 + x4 + x5, data = data)


Parameters:
   SVM-Type:  eps-regression 
 SVM-Kernel:  radial 
       cost:  1 
      gamma:  0.1111111 
    epsilon:  0.1 


Number of Support Vectors:  7
Command #207: rregress tweedie data=df y=yN x=x1,x2,x3,x4,x5
Call:
glm(formula = yN ~ x1 + x2 + x3 + x4 + x5, family = statmod::tweedie(var.power = 1.5, 
    link.power = 0), data = data)

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  5.132434   0.527029   9.738 0.000623 ***
x1          -0.024574   0.040256  -0.610 0.574543    
x2          -0.025566   0.008542  -2.993 0.040221 *  
x3           0.021695   0.037896   0.572 0.597637    
x4           0.026177   0.015205   1.722 0.160240    
x5          -0.049448   0.005687  -8.696 0.000963 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for Tweedie family taken to be 0.01253651)

    Null deviance: 7.59806  on 9  degrees of freedom
Residual deviance: 0.04988  on 4  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 3
Command #208: rregress tweedie data=df y=yN x=all
Call:
glm(formula = yN ~ yB + yC + x1 + x2 + x3 + x4 + x5, family = statmod::tweedie(var.power = 1.5, 
    link.power = 0), data = data)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  5.65435    1.15802   4.883    0.129
yB           0.23268    0.27465   0.847    0.553
yCB         -0.04918    0.10074  -0.488    0.711
yCC          0.20868    0.25592   0.815    0.565
x1          -0.15345    0.16156  -0.950    0.516
x2          -0.03311    0.01614  -2.051    0.289
x3           0.14596    0.15114   0.966    0.511
x4           0.03762    0.02610   1.441    0.386
x5          -0.06151    0.01670  -3.682    0.169

(Dispersion parameter for Tweedie family taken to be 0.02698886)

    Null deviance: 7.59806  on 9  degrees of freedom
Residual deviance: 0.02712  on 1  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 2
Command #209: rregress zeroinfl data=df y=yB x=x1,x2,x3,x4,x5
Call:
pscl::zeroinfl(formula = yB ~ x1 + x2 + x3 + x4 + x5, data = data)

Pearson residuals:
     Min       1Q   Median       3Q      Max 
-0.45947 -0.26794 -0.01359  0.09141  0.60865 

Count model coefficients (poisson with log link):
            Estimate Std. Error z value Pr(>|z|)
(Intercept) -9.15384   11.12547  -0.823    0.411
x1           1.13648    1.23371   0.921    0.357
x2          -0.15954    0.19555  -0.816    0.415
x3          -0.16241    1.13707  -0.143    0.886
x4          -0.34086    0.41293  -0.825    0.409
x5           0.08786    0.11902   0.738    0.460

Zero-inflation model coefficients (binomial with logit link):
             Estimate Std. Error z value Pr(>|z|)
(Intercept)   48.5403  3272.1665   0.015    0.988
x1            -2.9789   157.3541  -0.019    0.985
x2            -2.7745    29.5648  -0.094    0.925
x3            12.8501    65.4869   0.196    0.844
x4            -8.5312    84.0976  -0.101    0.919
x5            -0.8874    32.9442  -0.027    0.979

Number of iterations in BFGS optimization: 65 
Log-likelihood: -6.632 on 12 Df
Command #210: rregress zeroinfl data=df y=yB x=all
Call:
pscl::zeroinfl(formula = yB ~ yN + yC + x1 + x2 + x3 + x4 + x5, data = data)

Pearson residuals:
       Min         1Q     Median         3Q        Max 
-2.843e-09 -2.065e-09 -1.815e-09 -2.714e-11 -6.105e-22 

Count model coefficients (poisson with log link):
             Estimate Std. Error z value Pr(>|z|)
(Intercept)  87.09492         NA      NA       NA
yN           -5.56379         NA      NA       NA
yCB         -11.07250         NA      NA       NA
yCC         -34.45570         NA      NA       NA
x1           19.20183         NA      NA       NA
x2            2.02437         NA      NA       NA
x3          -26.54282         NA      NA       NA
x4            0.64897         NA      NA       NA
x5            0.09856         NA      NA       NA

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  746.693         NA      NA       NA
yN           -39.931         NA      NA       NA
yCB          -11.667         NA      NA       NA
yCC           41.080         NA      NA       NA
x1           -33.691         NA      NA       NA
x2            -2.848         NA      NA       NA
x3            30.959         NA      NA       NA
x4             5.995         NA      NA       NA
x5            -8.700         NA      NA       NA

Number of iterations in BFGS optimization: 1 
Log-likelihood:    -6 on 18 Df
Command #211: regress linear yN x1
       names      coef        se          T          pval        r2    adj_r2  CI[2.5%]  CI[97.5%]
0  Intercept  1.534998  0.375778   4.084848  3.510515e-03  0.961656  0.956863  0.668451   2.401545
1       None  0.343799  0.024272  14.164663  6.003703e-07  0.961656  0.956863  0.287828   0.399769
Command #212: regress linear y=yN x=x1 intercept=yes
       names      coef        se          T          pval        r2    adj_r2  CI[2.5%]  CI[97.5%]
0  Intercept  1.534998  0.375778   4.084848  3.510515e-03  0.961656  0.956863  0.668451   2.401545
1       None  0.343799  0.024272  14.164663  6.003703e-07  0.961656  0.956863  0.287828   0.399769
Command #213: regress logistic yB x1
       names      coef        se         z      pval  CI[2.5%]  CI[97.5%]
0  Intercept -0.006837  1.161436 -0.005887  0.995303 -2.283209   2.269535
1       None  0.032610  0.077732  0.419525  0.674832 -0.119741   0.184962
Command #214: regress logistic y=yB x=x1
       names      coef        se         z      pval  CI[2.5%]  CI[97.5%]
0  Intercept -0.006837  1.161436 -0.005887  0.995303 -2.283209   2.269535
1       None  0.032610  0.077732  0.419525  0.674832 -0.119741   0.184962
Command #215: read excel edata from data/random_5_samples.xlsx Sheet1
Read Excel: data/random_5_samples.xlsx.Sheet1 into edata
Command #216: let y_1 be list 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
y_1 = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0]
Command #217: regress linear y_1 edata
       names       coef         se         T      pval        r2    adj_r2   CI[2.5%]  CI[97.5%]
0  Intercept  10.077030  20.339927  0.495431  0.632167  0.455912  0.153642 -35.935082  56.089141
1    SampleA   0.115394   0.113597  1.015812  0.336248  0.455912  0.153642  -0.141582   0.372369
2    SampleB   0.001269   0.163862  0.007743  0.993991  0.455912  0.153642  -0.369413   0.371951
3    SampleC  -0.180128   0.087702 -2.053874  0.070173  0.455912  0.153642  -0.378523   0.018267
4    SampleD   0.004825   0.180605  0.026715  0.979270  0.455912  0.153642  -0.403732   0.413382
5    SampleE  -0.000058   0.097925 -0.000593  0.999540  0.455912  0.153642  -0.221581   0.221464
Command #218: regress linear y=y_1 x=edata intercept=yes
       names       coef         se         T      pval        r2    adj_r2   CI[2.5%]  CI[97.5%]
0  Intercept  10.077030  20.339927  0.495431  0.632167  0.455912  0.153642 -35.935082  56.089141
1    SampleA   0.115394   0.113597  1.015812  0.336248  0.455912  0.153642  -0.141582   0.372369
2    SampleB   0.001269   0.163862  0.007743  0.993991  0.455912  0.153642  -0.369413   0.371951
3    SampleC  -0.180128   0.087702 -2.053874  0.070173  0.455912  0.153642  -0.378523   0.018267
4    SampleD   0.004825   0.180605  0.026715  0.979270  0.455912  0.153642  -0.403732   0.413382
5    SampleE  -0.000058   0.097925 -0.000593  0.999540  0.455912  0.153642  -0.221581   0.221464
Command #219: let y_2 be list 1,0,1,0,1,0,1,0,1,0,1,0,1,0,1
y_2 = [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]
Command #220: regress logistic y_2 edata
       names         coef           se         z      pval     CI[2.5%]    CI[97.5%]
0  Intercept -1185.272924  1652.677301 -0.717184  0.473261 -4424.460913  2053.915064
1    SampleA    -3.815327     5.477507 -0.696545  0.486088   -14.551043     6.920388
2    SampleB     1.078366     2.183483  0.493874  0.621395    -3.201182     5.357913
3    SampleC    -0.906269     1.544008 -0.586959  0.557231    -3.932468     2.119931
4    SampleD    13.974954    19.605155  0.712820  0.475957   -24.450445    52.400352
5    SampleE     5.296718     7.422804  0.713574  0.475491    -9.251710    19.845146
Command #221: regress logistic y=y_2 x=edata
       names         coef           se         z      pval     CI[2.5%]    CI[97.5%]
0  Intercept -1185.272924  1652.677301 -0.717184  0.473261 -4424.460913  2053.915064
1    SampleA    -3.815327     5.477507 -0.696545  0.486088   -14.551043     6.920388
2    SampleB     1.078366     2.183483  0.493874  0.621395    -3.201182     5.357913
3    SampleC    -0.906269     1.544008 -0.586959  0.557231    -3.932468     2.119931
4    SampleD    13.974954    19.605155  0.712820  0.475957   -24.450445    52.400352
5    SampleE     5.296718     7.422804  0.713574  0.475491    -9.251710    19.845146
Command #222: let yN be clist 1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5
yN = [1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5]
Command #223: let yB be dlist 1, 0, 1, 0, 1, 0, 1, 1, 0, 1
yB = [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]
Command #224: let yC be slist A, B, C, A, B, C, A, B, C, A
yC = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A']
Command #225: let x1 be clist 2, 3, 5, 7, 11, 13, 17, 19, 23, 29
x1 = [2.0, 3.0, 5.0, 7.0, 11.0, 13.0, 17.0, 19.0, 23.0, 29.0]
Command #226: let x2 be clist 1, 4, 9, 16, 25, 36, 49, 64, 81, 100
x2 = [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]
Command #227: let x3 be clist 5, 8, 6, 10, 12, 14, 18, 20, 24, 30
x3 = [5.0, 8.0, 6.0, 10.0, 12.0, 14.0, 18.0, 20.0, 24.0, 30.0]
Command #228: let x4 be clist 3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3
x4 = [3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3]
Command #229: let x5 be clist 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
x5 = [100.0, 90.0, 80.0, 70.0, 60.0, 50.0, 40.0, 30.0, 20.0, 10.0]
Command #230: let df be dataframe yN:yN yB:yB yC:yC x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df = ['yN:yN', 'yB:yB', 'yC:yC', 'x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #231: let df_A be dataframe x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df_A = ['x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #232: let df_B be dataframe x1:x1 x2:x2
df_B = ['x1:x1', 'x2:x2']
Command #233: ttest 1s list x1 4
              T  dof alternative     p-val          CI95%   cohen-d  BF10     power
T-test  3.11881    9   two-sided  0.012342  [6.44, 19.36]  0.986254  5.24  0.792328
Command #234: ttest 1s list data=x1 mu=4
              T  dof alternative     p-val          CI95%   cohen-d  BF10     power
T-test  3.11881    9   two-sided  0.012342  [6.44, 19.36]  0.986254  5.24  0.792328
Command #235: ttest 1s dataframe wide df_B x1 5
               T  dof alternative     p-val          CI95%   cohen-d   BF10     power
T-test  2.768382    9   two-sided  0.021811  [6.44, 19.36]  0.875439  3.341  0.693589
Command #236: ttest 1s dataframe wide data=df_B.x1 mu=5
               T  dof alternative     p-val          CI95%   cohen-d   BF10     power
T-test  2.768382    9   two-sided  0.021811  [6.44, 19.36]  0.875439  3.341  0.693589
Command #237: ttest 2se list x1 x2
               T  dof alternative     p-val            CI95%   cohen-d   BF10     power
T-test -2.290406   18   two-sided  0.034287  [-49.08, -2.12]  1.024301  2.185  0.582133
Command #238: ttest 2se list data=x1,x2
               T  dof alternative     p-val            CI95%   cohen-d   BF10     power
T-test -2.290406   18   two-sided  0.034287  [-49.08, -2.12]  1.024301  2.185  0.582133
Command #239: ttest 2se dataframe wide df_B x1 x2
               T  dof alternative     p-val            CI95%   cohen-d   BF10     power
T-test -2.290406   18   two-sided  0.034287  [-49.08, -2.12]  1.024301  2.185  0.582133
Command #240: ttest 2se dataframe wide data=df_B.x1,df_B.x2
               T  dof alternative     p-val            CI95%   cohen-d   BF10     power
T-test -2.290406   18   two-sided  0.034287  [-49.08, -2.12]  1.024301  2.185  0.582133
Command #241: ttest 2su list x1 x2
               T  dof alternative     p-val            CI95%   cohen-d   BF10     power
T-test -2.290406   18   two-sided  0.034287  [-49.08, -2.12]  1.024301  2.185  0.582133
Command #242: ttest 2su list data=x1,x2
               T  dof alternative     p-val            CI95%   cohen-d   BF10     power
T-test -2.290406   18   two-sided  0.034287  [-49.08, -2.12]  1.024301  2.185  0.582133
Command #243: ttest 2su dataframe wide df_B x1 x2
               T  dof alternative     p-val            CI95%   cohen-d   BF10     power
T-test -2.290406   18   two-sided  0.034287  [-49.08, -2.12]  1.024301  2.185  0.582133
Command #244: ttest 2su dataframe wide data=df_B.x1,df_B.x2
               T  dof alternative     p-val            CI95%   cohen-d   BF10     power
T-test -2.290406   18   two-sided  0.034287  [-49.08, -2.12]  1.024301  2.185  0.582133
Command #245: ttest mwu list x1 x2
     U-val alternative     p-val   RBC  CLES
MWU   29.0   two-sided  0.121225  0.42  0.29
Command #246: ttest mwu list data=x1,x2
     U-val alternative     p-val   RBC  CLES
MWU   29.0   two-sided  0.121225  0.42  0.29
Command #247: ttest mwu dataframe wide df_B x1 x2
     U-val alternative     p-val   RBC  CLES
MWU   29.0   two-sided  0.121225  0.42  0.29
Command #248: ttest mwu dataframe wide data=df_B.x1,df_B.x2
     U-val alternative     p-val   RBC  CLES
MWU   29.0   two-sided  0.121225  0.42  0.29
Command #249: ttest paired list x1 x2
               T  dof alternative    p-val            CI95%   cohen-d  BF10    power
T-test -3.209935    9   two-sided  0.01066  [-43.64, -7.56]  1.024301  5.89  0.82128
Command #250: ttest paired list data=x1,x2
               T  dof alternative    p-val            CI95%   cohen-d  BF10    power
T-test -3.209935    9   two-sided  0.01066  [-43.64, -7.56]  1.024301  5.89  0.82128
Command #251: ttest paired dataframe wide df_B x1 x2
               T  dof alternative    p-val            CI95%   cohen-d  BF10    power
T-test -3.209935    9   two-sided  0.01066  [-43.64, -7.56]  1.024301  5.89  0.82128
Command #252: ttest paired dataframe wide data=df_B.x1,df_B.x2
               T  dof alternative    p-val            CI95%   cohen-d  BF10    power
T-test -3.209935    9   two-sided  0.01066  [-43.64, -7.56]  1.024301  5.89  0.82128
Command #253: ttest tost list x1 x2
      bound  dof      pval
TOST      1   18  0.979484
Command #254: ttest tost list data=x1,x2
      bound  dof      pval
TOST      1   18  0.979484
Command #255: ttest tost dataframe wide df_B x1 x2
      bound  dof      pval
TOST      1   18  0.979484
Command #256: ttest tost dataframe wide data=df_B.x1,df_B.x2
      bound  dof      pval
TOST      1   18  0.979484
Command #257: let yN be clist 1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5
yN = [1.2, 2.3, 3.1, 4.8, 5.6, 6.2, 7.9, 8.4, 9.7, 10.5]
Command #258: let yB be dlist 1, 0, 1, 0, 1, 0, 1, 1, 0, 1
yB = [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]
Command #259: let yC be slist A, B, C, A, B, C, A, B, C, A
yC = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A']
Command #260: let x1 be clist 2, 3, 5, 7, 11, 13, 17, 19, 23, 29
x1 = [2.0, 3.0, 5.0, 7.0, 11.0, 13.0, 17.0, 19.0, 23.0, 29.0]
Command #261: let x2 be clist 1, 4, 9, 16, 25, 36, 49, 64, 81, 100
x2 = [1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]
Command #262: let x3 be clist 5, 8, 6, 10, 12, 14, 18, 20, 24, 30
x3 = [5.0, 8.0, 6.0, 10.0, 12.0, 14.0, 18.0, 20.0, 24.0, 30.0]
Command #263: let x4 be clist 3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3
x4 = [3.1, 5.2, 2.7, 8.6, 9.1, 4.4, 7.8, 6.5, 10.2, 11.3]
Command #264: let x5 be clist 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
x5 = [100.0, 90.0, 80.0, 70.0, 60.0, 50.0, 40.0, 30.0, 20.0, 10.0]
Command #265: let df be dataframe yN:yN yB:yB yC:yC x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df = ['yN:yN', 'yB:yB', 'yC:yC', 'x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #266: let df_A be dataframe x1:x1 x2:x2 x3:x3 x4:x4 x5:x5
df_A = ['x1:x1', 'x2:x2', 'x3:x3', 'x4:x4', 'x5:x5']
Command #267: let df_B be dataframe x1:x1 x2:x2
df_B = ['x1:x1', 'x2:x2']
Command #268: variance bartlett list x1 x2 x3 x4 x5
Statistic = 50.422; p-value = 2.9471902944178116e-10
Command #269: variance bartlett list data=x1,x2,x3,x4,x5
Statistic = 50.422; p-value = 2.9471902944178116e-10
Command #270: variance bartlett frame wide df_A
Statistic = 50.422; p-value = 2.9471902944178116e-10
Command #271: variance bartlett dataframe wide data=df_A
Statistic = 50.422; p-value = 2.9471902944178116e-10
Command #272: variance fligner list x1 x2 x3 x4 x5
Statistic = 26.972; p-value = 2.0136953020687342e-05
Command #273: variance fligner list data=x1,x2,x3,x4,x5
Statistic = 26.972; p-value = 2.0136953020687342e-05
Command #274: variance fligner frame wide df_A
Statistic = 26.972; p-value = 2.0136953020687342e-05
Command #275: variance fligner dataframe wide data=df_A
Statistic = 26.972; p-value = 2.0136953020687342e-05
Command #276: variance levene list x1 x2 x3 x4 x5
Statistic = 12.206; p-value = 8.398779618728844e-07
Command #277: variance levene list data=x1,x2,x3,x4,x5
Statistic = 12.206; p-value = 8.398779618728844e-07
Command #278: variance levene frame wide df_A
Statistic = 12.206; p-value = 8.398779618728844e-07
Command #279: variance levene dataframe wide data=df_A
Statistic = 12.206; p-value = 8.398779618728844e-07
